
%----------------------------------------------------------------------------------------
%	Lecture 2
%----------------------------------------------------------------------------------------

\chapter{Numerical Methods}

\bigbreak

Given $y' = f(x, y)$ and $y(x_0) = y_0$, we want to find the curve that satisfies these equations.
Together these are called Initial Value Problem (IVP).

\section{Euler's Method}

Euler used this method to prove that the existence and uniqueness method.
At the point $(x_0, y_0)$, the only information we have is the direction of the tangent.

Here, we choose a step size $h$.
We calculate the next point $(x_0 + h, y_1)$ according to the tangent line.
Then we recalculate the slope at the new point and repeat.

Now, we want derive the equations for this method.
Let's say after $n$ iterations you are at the point $(x_n, y_n)$ and the slope at that point is $A_n$.
So if we move from $x_n$ to $x_{n+1} = x_n + h$ then the $y_{n+1} = y_n + A_n h$.
So in general,
$$ y_{n+1} = y_n + h A_n ; \quad x_{n+1} = x_n + h; \quad A_n = f(x_n, y_n) $$

These are called the Euler's Method's Equations.
Here, if the curve is concave up then the Euler's approximation is lower than the actual value.
And if the curve is concave down then the Euler's approximation is higher than the actual value.

You can determine the concavity of the curve at the starting point by calculating the seocnd derivative of the curve.
If the curve changes concavity then it is difficult to compare the values of Euler's approximation and the actual value.
So these comparisions will only work nearby the starting point.

\subsection{Error Approximation}

Here, we defined the error as the absolute value of the difference between the actual curve and the approximation.
For Euler's Method, asymptotically $err \sim c h$ where $h$ is the step size.

Since the error is proportional to first power of $h$ so Euler's method is called the First Order Method.
Thus, if you halve the step size then you halve the error.

\pagebreak

\section{Improved Euler's Method}

Here, we will use the average of two slopes $A_n$ and $B_n$ as the slope for our approximation.
Here, $A_n$ is the slope at $(x_n, y_n)$ and $B_n$ is the slope at $(x_n + h, y_n + h A_n)$.
That is, $B_n$ is the slope at the next point if we follow the slope $A_n$.

\begin{gather*}
	x_{n+1} = x_n + h \\
	y_{n+1} = y_n + h \left( \frac{A_n + B_n}{2} \right) \\
	A_n = f(x_n, y_n) \\
	B_n = f(x_n + h, y_n + h A_n)
\end{gather*}


This method is also called {\bf Heun's Method} or {\bf Modifeid Euler's Method} or {\bf RK2 Method}.
{\bf RK} stands for {\bf Runge Kutta} and the two is because this is a second order method.
That is, $err \sim c h^2$.

This methods seems better but in practice this is less efficient because you have to evaluate the function twice.

In general, the number of function evaluation will give you the order of the approximation.
There is a method {\bf RK4} which will have four function evaluations and it is a fourth order method.

\section{Pitfalls of Numerical Methods}

The curve $y = \frac{1}{c-x}$ is a solution to $y' = y^2$.
If we take $c = 1$ and start at the point $(0, 1)$ and ask the computer to find the value at $x = 2$,
it will never reach the value because there is an infinity in between.

We cannot predict the point at which the function is going to infinity because it depends on the constant of integration and each curve has its own unique point where it blows up.
